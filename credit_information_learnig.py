# -*- coding: utf-8 -*-
"""credit information learnig.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hgaUBviq9p2pPuMyRGgDSKOvpGqopeV0
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
# %matplotlib inline

df=pd.read_csv('application_train.csv')
df.head()

df.shape

total= df.isnull().sum ().sort_values(ascending=False)
percent=(df.isnull().sum ()/df.isnull().count()).sort_values(ascending=False)
data =pd.concat([total,percent],axis=1, keys=['Total','percentage'])
data

features=data[data.percentage <0.1]
features

features.index

df1=df[features.index]
df1.head()

tg=df["TARGET"].value_counts()
plt.subplots(figsize=(12,8))
pie_target=tg.plot.pie(autopct="%1f%%")
pie_target.legend(loc=1,labels={'client with payment+difficulties:0','All other cases : 1'})
plt.show()

def  dummy_variable_all(df,sort_frequence=True,dropna=True,map_show=False,skip_column=[]):
     for column_name in df:
         if np.dtype(df[column_name])=="O"and column_name not in skip_column:
            if  sort_frequence:
                unique_value = df[column_name].value_counts(dropna=dropna).sort_values().index
            else:
                unique_value = df[column_name].value_counts(dropna=dropna).sort_index().index
            name_map={}
            for i, value in enumerate(unique_value):
                name_map[value]=i
            if map_show:
               print('column_name :',column_name)
               print ('replace:',name_map)
            df[column_name]=df[column_name].map(name_map)
     return df

df2=dummy_variable_all(df.copy(),dropna=False,map_show=True)
df2.fillna(df2.median(),inplace=True)

pd.set_option('display.max_columns',None)
df2

df2.dtypes.value_counts()

corr=df2.corr()
plt.subplots(figsize=(18,12))
sns.heatmap(corr,linewidth=0.1)
plt.show()

target_corr=abs (corr['TARGET'].copy().drop(index=['TARGET'])).sort_values(ascending=False)
target_corr

x=df2[df2.columns.copy().drop('TARGET')].values
y=df2['TARGET'].values 
print(x.shape,y.shape)

from sklearn.feature_selection import SelectKBest,f_classif
x_select=SelectKBest(f_classif , k=2).fit_transform(x,y)
print(x_select)

selected_features=[]
for i in df2.columns:
    if all(df2.loc[:,i]==x_select[:,0]) or all(df2.loc[:,i]==x_select[:,1]):
        selected_features.append(i)
print(selected_features)

objects=df1.select_dtypes(include=[object])
objects

objects.isna().sum()

df1['NAME_TYPE_SUITE'].value_counts()

from sklearn.model_selection import train_test_split
x_train , x_test,y_train,y_test=train_test_split(x_select,y,train_size=0.75,test_size=0.25)
print(x_train.shape,x_test.shape,y_train.shape,y_test.shape)

scaler=StandardScaler()
scaler.fit(x_train)
x_train_scaled=scaler.transform(x_train)
x_test_scaled=scaler.transform(x_test)
print(x_train)
print(x_train_scaled)
print(x_test)
print(x_test_scaled)

from sklearn.metrics import *
def evaluation(test,predict):
  acc_score=accuracy_score(test,predict)
  prec_score=precision_score(test,predict)
  rec_score=recall_score(test,predict)
  f_score=f1_score(test,predict)
  conf_matrix=confusion_matrix(test,predict)
  print('Accuracy:{:.3f}'.format(acc_score))
  print('Precision:{:.3f}'.format(prec_score))
  print('Recall:{:.3f}'.format(rec_score))
  print('F:{:.3f}'.format(f_score))
  print('Confussion matrix: in',conf_matrix)

from sklearn.neighbors import KNeighborsClassifier
neigh=KNeighborsClassifier(n_neighbors=3)
neigh.fit(x_train_scaled,y_train)
predict_3=neigh.predict(x_test_scaled)
evaluation(y_test,ppredict_3)

from sklearn.tree import DecisionTreeClassifier
dtree=DecisionTreeClassifier()
dtree.fit(x_train_scaled,y_train)
dtree_predict=dtree.predict(x_test_scaled)
evaluation(y_test,dtree_predict)

from sklearn.ensemble import RandomForestClassifier
rforest=RandomForestClassifier()
rforest.fit(x_train_scaled,y_train)
rforest_predict=rforest.predict(x_test_scaled)
evaluation(y_test,rforest_predict)

neigh_prob=neigh.predict_proba(x_test_scaled)
print("KNeighborsClassifier probas :")
print(neigh_prob)

dtree_prob=dtree.predict_proba(x_test_scaled)
print("DecisionTreeClassifier probes :")
print(dtree_prob)

rforest_prob=rforest.predict_proba(x_test_scaled)
print("RandomForestClassifier probe:")
print(rforest_prob)

average_prob=(neigh_prob+ dtree_prob+ rforest_prob)/3
average_prob

df_test=pd.read_csv('application_test.csv')
df_test

x_test_file=df_test[selected_features].values

x_test_file_scaled=scaler.transform(x_test_file)
x_test_scaled

x_test_scaled_finite=x_test_file_scaled[np.all(np.isfinite(x_test_file_scaled),axis=1)]
neigh_prob=neigh.predict_proba(x_test_scaled_finite)
dtree_prob=dtree.predict_proba(x_test_scaled_finite)
rforest_prob=rforest.predict_proba(x_test_scaled_finite)
average_prob=(neigh_prob+ dtree_prob+ rforest_prob)/3
predict_res=1-average_prob[:,0]

pred=pd.Series(predict_res,name='TARGET')
final=pd.concat([df_test['SK_ID_CURR'],pred],axis=1)
final

final.to_csv('final_output_csv_file.csv',index=False)

pd.read_csv('final_output_csv_file.csv')

